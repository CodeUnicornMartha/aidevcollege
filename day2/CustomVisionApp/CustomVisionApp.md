# AI Developer College Day2 - Custom Vision

As we have looked at Azure Machine Learning Service, MLOps, Cognitive Search, Cognitive Services so far and saw how we can enrich our applications with machine learning. As a developer I can use those REST APIs and enrich my applications without having deep data scientist knowledge. As final challenge we have prepared a small application which adopts a pre-trained object detection model. This shows the entire lifecycle and brings together best of both worlds.

# Using an Azure Custom Vision Object Detection model to detect Logos in a web browser (using TensorFlow)

This example shows you how to use a Machine Learning, which was created with the [Microsoft Azure Custom Vision](https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/) service, in a web browser application.

The [Azure Logo images](https://github.com/microsoft/AIVisualProvision/tree/master/Documents/Images/Training_DataSet) used in this example, are from the AI Vision Provision demo shown at the Microsoft Connect() event in 2018.

In this challenge we will use a pretrained model which is already downloaded and run the sample code to detect logos on an uploaded image. For explanation purposes the `Training, Testing & Downloading Model part` is explained. Afterwards you will open [Visual Studio Code](https://code.visualstudio.com/docs/setup/setup-overview) using [Node.js](https://nodejs.org/en/download/) and run `npm install` as well as `node server.js` to start the project. At `http://localhost:3000` you can then test the model by uploading some of the [Azure Logo images](https://github.com/microsoft/AIVisualProvision/tree/master/Documents/Images/Training_DataSet).

## Explanation: Training, Testing and Downloading the Model

There are two options, either to choose the __Classification__ or the __Object Detection__ model. <br>
In this case to detect the objects on an image the `Object Detection Model` was chosen. For downloading purposes we chose the `General (Compact)` domain which is slightly less accurate than a standard domain with the same amount of training data.


Custom Vision Service only exports __compact__ domains. The models generated by compact domains are optimized for the constraints of real-time classification on mobile devices. Classifiers built with a compact domain may be slightly less accurate than a standard domain with the same amount of training data.
For information on improving your classifiers, see the [Improving your classifier](getting-started-improving-your-classifier.md) document. For information on improving your classifiers, see the Improving your classifier document.
<br><br>

![Object Detection](./images/ObjectDetection.png)

As a minimum, we recommend you use at least 30 images per tag in the initial training set. You'll also want to collect a few extra images to test your model once it's trained.

In order to train your model effectively, use images with visual variety. Select images that vary by:
- camera angle
- lighting
- background
- visual style
- individual/grouped subject(s)
- size
- type

Additionally, make sure all of your training images meet the following criteria:
- .jpg, .png, .bmp, or .gif format
- no greater than 6MB in size (4MB for prediction images)
- no less than 256 pixels on the shortest edge; any images shorter than this will be automatically scaled up by the Custom Vision Service

It's very important to tag every instance of the object(s) you want to detect, because the detector uses the untagged background area as a negative example in training. When you're done tagging, click the arrow on the right to save your tags and move on to the next image.
Labeled pictures to annotate the objects to train the model are shown below.

![Train Images](./images/TrainImages.png)

![Test Images](./images/TrainImages2.png)

For training purposes - the **quick training** or the **advanced training** can be chosen.
Use advanced training for improved performance, especially on challenging and fine-grained datasets.
With advanced training, you can specify a compute time budget for training and Custom Vision will experimentally identify the best training and augmentation settings. For quick iterations, you can use the quick training.

![Training Process](./images/TrainProcess.png)

After training has completed, the model's performance is estimated and displayed. The Custom Vision Service uses the images that you submitted for training to calculate **precision** and **recall**, using a process called k-fold cross validation. Precision and recall are two different measurements of the effectiveness of a classifier:

Precision indicates the fraction of identified classifications that were correct. For example, if the model identified 100 images as dogs, and 99 of them were actually of dogs, then the precision would be 99%.
Recall indicates the fraction of actual classifications that were correctly identified. For example, if there were actually 100 images of apples, and the model identified 80 as apples, the recall would be 80%.

![Training Results](./images/TrainResultIteration1.png)

Then the model is tested using the Quick Test as shown in the screenshots below:

![Quick Test](./images/QuickTest.png)

![Quick Test](./images/QuickTest2.png)

Then the Plattform will be chosen, you can read more detail in the [documentation](https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/export-your-model):

![Choose a Plattform](./images/ChoosePlattform.png)

And finally the Model is downloaded, in this case a Tensorflow model as we use the Tensorflow framework:

![Download Model](./images/DownloadModel.png)

## Start the Challenge: Open the Custom Vision App in Visual Studio Code

First we open up the Code in Visual Studio Code:

![Open the Code](./images/VisualStudioCode.png)

**Let's look into the code:** <br>

In the `predict.js` file there is the `Load Graph Model` Function to load the machine learning model and point it to the model.json file in the model folder:

![Load Model](./images/loadModel.png)

From the custom vision service we get the `detected boxes`, `detected classes` and `detected scores` in return.
As previously shown objects are annotated by boxes, classes are detected and scores are predicted.

![Returned from Service](./images/ReturnService.png)

Further we receive the `target classes` structured in indexes and labels. Those are the labels which are shown on detected objects. 

![Indexes Labels](./images/IndexesLabels.png)

To sum it up images are annotated by boxes, labeled with classes and receive a score to what percentage an object was detected.
In the following we have 2 Options, either you can run the application locally or you can use a pre-deployed option and test it with the sample [images](https://github.com/microsoft/AIVisualProvision/tree/master/Documents/Images/Training_DataSet) as previously stated.

## OPTION 1: Use a pre-deployed Application

If you want, you can test the deployed application under [https://tfjs-objectdetection.azureedge.net](https://tfjs-objectdetection.azureedge.net).

## OPTION 2: Setup your own Application

As prerequisite you should install [Node.js](https://nodejs.org/en/download/) and [Visual Studio Code](https://code.visualstudio.com/docs/setup/setup-overview).

Begin by opening a **Terminal** (a Bash console, Powershell console or other environment in which you've installed Node.js):

1. Create a development directory, giving it the name `aicollege` :

    ```bash
    mkdir aicollege
    cd aicollege
    git clone https://github.com/aidevcollege/aidevcollege.git
    cd day2/CustomVisionApp # Navigate `cd` to the source code folder
    code .
    ```

1. Install the packages for the sample with `npm install`.  This command will download the packages upon which the code depends.
1. Run the local web server script: `node server.js` and examine the results

**Final Result**

Finally you can test the model at `http://localhost:3000` by uploading some of the [Azure Logo images](https://github.com/microsoft/AIVisualProvision/tree/master/Documents/Images/Training_DataSet). <br>

This is how the model should the detect the objects as shown below:

![Final Results](./images/FinalResult.png)

![Final Results](./images/FinalResult2.png)

<br>
<br>

> **That's officially it for today - Well done! :clap:**

<br>
<br>

### Bonus Challenge - Want more?

If you have chosen Option 1 and want to run the application as a Github Pages or as a static Website you can jump either to [Option A - Github Pages](#option-a---github-pages) or [Option B - Static Website](#option-b---static-website).

### Option A - Github Pages

**What are Github Actions?**
GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. If you want more detail you can read the following [documentation](https://github.com/features/actions)

In this repository there is a github action included. You can run the `Github Action` on the root level manually using the Github User Interface.

![GithubActionsPages](./images/GithubActionsPages.png)

Be sure to enable and **save** `Github Pages` underneath the `Settings`, beneath `Options` and select **Branch:** `gh-pages:`

![EnableGithubPages](./images/EnableGithubPages.png)

Finally this application is hosted by Github Pages:

![FinalResultCustomVisionGithubPages](./images/FinalResultCustomVisionGithubPages.png)

### Option B - Static Website

If you want to create your own `static website` by deploying the application on a (previously created) Storage Account, we have added another `Github Action` in the folder `day2/CustomVisionApp/.github/workflows`. Read the links below how to get this sample running:

If you create a `service principal` and a [`secret from a pre deployed storage account`](https://github.com/Azure/actions-workflow-samples/blob/master/assets/create-secrets-for-GitHub-workflows.md), you can use the [`static website`](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blobs-static-site-github-actions) feature from the storage account. The documentation on how to complete the tasks are included in the links.


### References
 YouTube Video: https://www.youtube.com/watch?v=7gOYpT732ow&list=PLZk8J6FocZbaClHkIPk4SWZHxn_9VArb5&index=2